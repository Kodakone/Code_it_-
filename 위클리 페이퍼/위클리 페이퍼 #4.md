Weekly_Paper #4
=============

> # 딥러닝과 머신러닝 간의 포함관계에 대해 설명해주세요.

### 간단히 말하자면, 신경망의 깊이를 얼마나 더 깊게 다루느냐 / 알아서 학습을 하느냐에 따라 차이가 있다. [딥러닝 ⊆ 머신러닝] 
딥러닝과 머신러닝의 포함관계는 딥러닝이 머신러닝의 한 부분이라고 볼 수 있다. 
머신러닝은 데이터를 기반으로 스스로 학습하거나 예측하여 의사 결정을 내리는 알고리즘 / 기술을 의미한다.
이때, 학습 등에 대해 사람이 개입하여 특징을 설계 해주어야 할 필요가 있다.

딥러닝은 머신러닝의 한 영역으로, 신경망 구조를 기반으로 데이터에서 스스로 특징 추출 및 학습을 진행한다. 
이는 즉, 사람이 직접 개입 할 필요 없이, 대량의 data를 처리할 수 있는 모델을 의미한다. 

일반적으로 머신러닝보다도 딥러닝의 신경망 층이 더 깊고(머신러닝이 3~4개의 층이라면 딥러닝은 그보다 매우 많은 층으로 이루어져 있음),
복잡한 데이터 및 문제를 해결하기 위해 인공신경망을 더 심화하여 활용하는 기술이 바로 딥러닝이다. 
따라서, 머신러닝은 딥러닝을 포함한 개념으로, 딥러닝은 머신러닝의 포함관계이다. 

------------------------------------------
> # 딥러닝의 성능향상을 위해 고려하는 하이퍼파라미터의 종류에는 어떤 것들이 있는지 설명해주세요.

#### Hyperparameter: 학습하기 전, 미리 정해줘야 하는 변수 또는 파라미터들을 의미
딥러닝 성능을 향상을 위해 고려하는 하이퍼파라미터는 모델 구조, 학습 과정, 정규화 관련 하이퍼파라미터가 있다. 

모델 구조: 
+ 신경망 Layer 및 신경망 자체의 수(모델의 깊이) - 너무 깊으면 overfitting / 얕으면 underfitting. 적절히 설정 필요 
+ 활성 함수(ReLU, Sigmoid, Tanh 등) - 데이터 및 모델, 문제 해결에 알맞은 함수 설정 필요

학습 과정: 
+ 학습률 - 모델의 학습 속도 설정. 너무 크면 발산하여 목표 성능에 도달 X / 작으면 학습 속도 느림
+ 배치 크기 - 한 번의 학습에서 처리하는 데이터 샘플 수. 학습 속도와 모델의 안정성과 연관되어 적절히 설정 필요

정규화:
+ Dropout - 과적합을 방지하기 위해 일부 신경망 랜덤 제거. 과하게 설정 시 오히려 학습 성능 저하
+ L1/L2 Regulization - 가증치 크기 제한으로 모델의 복잡도 감소. 필요시 설정하여 모델 안정화
